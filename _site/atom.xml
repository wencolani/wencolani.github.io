<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.5.2">Jekyll</generator><link href="http://localhost:4000/atom.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2017-11-07T18:19:20+01:00</updated><id>http://localhost:4000/</id><title type="html">wencolani</title><subtitle>a CS Phd candidate in Zhejiang University</subtitle><author><name>wencolani</name></author><entry><title type="html">Global RDF Vector Space Embeddings</title><link href="http://localhost:4000/paper%20reading/2017/11/04/PaperReading/" rel="alternate" type="text/html" title="Global RDF Vector Space Embeddings" /><published>2017-11-04T00:00:00+01:00</published><updated>2017-11-04T00:00:00+01:00</updated><id>http://localhost:4000/paper%20reading/2017/11/04/PaperReading</id><content type="html" xml:base="http://localhost:4000/paper%20reading/2017/11/04/PaperReading/">&lt;p&gt;这篇文章提出了一个结合GloVe想法的RDF2Vec新方法。&lt;/p&gt;

&lt;h1 id=&quot;motivation&quot;&gt;Motivation&lt;/h1&gt;
&lt;p&gt;RDF2Vec 方法只用了local pattern。&lt;/p&gt;

&lt;p&gt;新提出的方法可以有效利用global patterns。&lt;/p&gt;

&lt;h1 id=&quot;method&quot;&gt;Method&lt;/h1&gt;

&lt;h2 id=&quot;building-a-co-occurrence-matrix-for-graph-data&quot;&gt;building a co-occurrence matrix for graph data&lt;/h2&gt;

&lt;p&gt;在GloVe首先构造了word co-occurrence matrix, 本文的新方法首先对RDF数据构造了co-occurrence matrix。&lt;/p&gt;

&lt;p&gt;最直观的构造方法：从任意节点出发，进行固定步数的深度搜索，将所有限定步数以内可达的节点均当作出发节点的context，并且对越远的节点赋予越低的权重。但是这样的方法主要有以下问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;同一个节点可能通过不同长度的路径被达到&lt;/li&gt;
  &lt;li&gt;路径中可能存在环状结构&lt;/li&gt;
  &lt;li&gt;各个不同长度的可达节点数分布不均&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;所以本文采取了Personalized PageRank 来衡量其他节点作为某一节点的context的重要性。&lt;/p&gt;

&lt;h1 id=&quot;paper-information&quot;&gt;Paper information&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Global RDF Vector Space Embeddings&lt;/li&gt;
  &lt;li&gt;Michael Cochez, Petar Ristoski, Simone Paolo Ponzetto, Heiko Paulheim2&lt;/li&gt;
  &lt;li&gt;ISWC 2017&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://iswc2017.semanticweb.org/wp-content/uploads/papers/MainProceedings/164.pdf&quot;&gt;article link&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/miselico/globalRDFEmbeddingsISWC&quot;&gt;code link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>wencolani</name></author><category term="Paper reading" /><category term="paper" /><category term="kg embedding" /><category term="rdf2vec" /><category term="glove" /><summary type="html">这篇文章提出了一个结合GloVe想法的RDF2Vec新方法。</summary></entry><entry><title type="html">Towards Holistic Concept Representations, Embedding Relational Knowledge, Visual Attributes, and Distributional Word Semantics</title><link href="http://localhost:4000/paper%20reading/2017/11/03/PaperReading/" rel="alternate" type="text/html" title="Towards Holistic Concept Representations, Embedding Relational Knowledge, Visual Attributes, and Distributional Word Semantics" /><published>2017-11-03T00:00:00+01:00</published><updated>2017-11-03T00:00:00+01:00</updated><id>http://localhost:4000/paper%20reading/2017/11/03/PaperReading</id><content type="html" xml:base="http://localhost:4000/paper%20reading/2017/11/03/PaperReading/">&lt;p&gt;这篇文章提出了一个融合kg embedding，image embedding 和 word embedding的方法。能够融合更多的信息。&lt;/p&gt;

&lt;h1 id=&quot;motivation&quot;&gt;Motivation&lt;/h1&gt;
&lt;p&gt;知识库，图像和文字这三种方式分别记录了事物不同方面的信息。
其中知识库主要包含了这个世界上跨领域的、通用的知识，侧重于记录实体之间的关系信息。
图片记录了视觉信息，例如颜色和形状等。
而文字记录了一些知识库中不包含的信息，知识库中记录的都是相对稳定的关系信息。&lt;/p&gt;

&lt;p&gt;这篇文章的目的是提出一种tri-modal的embedding学习方法，能够有效融合和利用三种不同的信息。&lt;/p&gt;

&lt;h1 id=&quot;method&quot;&gt;Method&lt;/h1&gt;
&lt;p&gt;方法的整体思路如下：
&lt;img src=&quot;/img/2017-11-03-method.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;word-embedding&quot;&gt;Word Embedding:&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;method: word2vec.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;kg-embedding&quot;&gt;KG Embedding：&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;data: DBpedia&lt;/li&gt;
  &lt;li&gt;method: TransE&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;visual-object&quot;&gt;Visual Object：&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;data: ImageNet 1k (1000 categories, each with &amp;gt; 1300 images), 这些图片是和WordNet中的synset对应的。&lt;/li&gt;
  &lt;li&gt;method：Inception-V3&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;shared-concept-spacefusion&quot;&gt;Shared concept space(fusion):&lt;/h2&gt;
&lt;p&gt;每一个concept都有来自KG， 文本，图片的向量表示。融合三个表示的方法主要有以下几种：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;AVG：在计算相似度的场景下，分别计算和三个不同向量表示的相似度然后平均值。&lt;/li&gt;
  &lt;li&gt;CONC：先concatinate三个向量表示，然后在计算相似度。&lt;/li&gt;
  &lt;li&gt;SVD&lt;/li&gt;
  &lt;li&gt;PCA&lt;/li&gt;
  &lt;li&gt;AUTO：autoencoder&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt;

&lt;h2 id=&quot;word-siilarity&quot;&gt;word siilarity&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/img/2017-11-03-result1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;entity-segmentation&quot;&gt;entity segmentation&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/img/2017-11-03-result2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;entity-type-prediction&quot;&gt;entity-type prediction&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/img/2017-11-03-result3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;thinking&quot;&gt;Thinking&lt;/h1&gt;
&lt;ol&gt;
  &lt;li&gt;多源融合的embedding是值得研究的方向。&lt;/li&gt;
  &lt;li&gt;如何将多源的信息在同一个向量空间中表示也是可以研究的点。&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;paper-information&quot;&gt;Paper information&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Towards Holistic Concept Representations: Embedding Relational Knowledge, Visual Attributes, and Distributional Word Semantics&lt;/li&gt;
  &lt;li&gt;Steffen Thoma, Achim Rettinger, and Fabian Both&lt;/li&gt;
  &lt;li&gt;ISWC 2017&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://iswc2017.semanticweb.org/wp-content/uploads/papers/MainProceedings/260.pdf&quot;&gt;article link&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;code link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>wencolani</name></author><category term="Paper reading" /><category term="paper" /><category term="kg embedding" /><category term="word2vec" /><category term="image embedding" /><category term="embedding" /><category term="image" /><category term="text" /><category term="NLP" /><summary type="html">这篇文章提出了一个融合kg embedding，image embedding 和 word embedding的方法。能够融合更多的信息。</summary></entry><entry><title type="html">cat</title><link href="http://localhost:4000/linux%20commond/2017/11/03/LinuxCommond/" rel="alternate" type="text/html" title="cat" /><published>2017-11-03T00:00:00+01:00</published><updated>2017-11-03T00:00:00+01:00</updated><id>http://localhost:4000/linux%20commond/2017/11/03/LinuxCommond</id><content type="html" xml:base="http://localhost:4000/linux%20commond/2017/11/03/LinuxCommond/">&lt;h2 id=&quot;main-commond&quot;&gt;main commond&lt;/h2&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cat file1&lt;/code&gt;: output the content in file1&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cat file1 &amp;gt; file2&lt;/code&gt;: rewrite file1 to file2&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cat file1 | less&lt;/code&gt;: output the content in file1 in less way&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cat &amp;gt; file1&lt;/code&gt;: write to file1, if file1 exists, the content will be covered. Use &lt;code class=&quot;highlighter-rouge&quot;&gt;Ctrl + d&lt;/code&gt; to end input&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cat &amp;gt;&amp;gt; file1&lt;/code&gt;: append new written content to file1&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cat file1 file2 file3&lt;/code&gt;: output file1, file2 and file3&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cat file1 file2 file3 &amp;gt; file4&lt;/code&gt;: write file1, file2 and file to file4&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cat file1 file2 file3 | sort &amp;gt; file4&lt;/code&gt;: sort all the content in file1, file2 and file3, then write them to file4&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cat - file1 &amp;gt; file2&lt;/code&gt;: first copy file1 to file2, then add input something to file2&lt;/p&gt;

&lt;h2 id=&quot;option&quot;&gt;option&lt;/h2&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;-n&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;--number&lt;/code&gt;: 从0开始对每一行编号&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;-b&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;--number-nonblank&lt;/code&gt;: 对空白行不编号&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;-s&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;--squeeze-blank&lt;/code&gt;: 当有连续两行空白行时换成一行&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;</content><author><name>wencolani</name></author><category term="Linux commond" /><category term="linux commond" /><category term="cat" /><summary type="html">main commond cat file1: output the content in file1</summary></entry><entry><title type="html">Distributed Representations of Sentences and Documents</title><link href="http://localhost:4000/paper%20reading/2017/11/02/PaperReading/" rel="alternate" type="text/html" title="Distributed Representations of Sentences and Documents" /><published>2017-11-02T00:00:00+01:00</published><updated>2017-11-02T00:00:00+01:00</updated><id>http://localhost:4000/paper%20reading/2017/11/02/PaperReading</id><content type="html" xml:base="http://localhost:4000/paper%20reading/2017/11/02/PaperReading/">&lt;p&gt;这篇文章提出了一个非监督学习方法Paragraph Vectore，从文本中学习句子，段落，和文本的向量表示。&lt;/p&gt;

&lt;h1 id=&quot;motivation&quot;&gt;Motivation:&lt;/h1&gt;
&lt;p&gt;很多机器学习方法要求输入是固定长度的特征向量，词袋(bag-of-word)是最常用的方法，但词袋有两个最重要的缺点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;失去了词序信息。虽然bag-of-n-gram考虑了词序的信息，但是会面临新的问题：data-sparsity和 high-dimensionality。&lt;/li&gt;
  &lt;li&gt;忽略了词的语义信息(semantics of words)。比如“苹果”和“梨”。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;adavantages-of-paragraph-vectore&quot;&gt;Adavantages of Paragraph Vectore&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Pagraph vector 是一个非监督的模型。可以应用于标注不足的数据。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Paragraph Vectore 是固定长度的向量，它表示的文本长度是可变的，可以从sentence到paragraph到text。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Paragraph vector acts a memory that remembers that is missing from the current context – or the topic of the paragraph. 我的理解是在预测文本中的下一个词时，通常会取这个词前后的固定长度的词(context)，虽然多数情况下, context是非常有效的信息，但有时候这些词对于预测所需要的信息的覆盖是补全的，这就是paragraph vector存在的意义，它可以捕捉并记住文本的主题信息，并辅助预测。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;method&quot;&gt;Method:&lt;/h1&gt;

&lt;h2 id=&quot;paragraph-vector-a-distributed-memory-modelpv-dm&quot;&gt;Paragraph Vector: a distributed memory model(PV-DM)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/img/2017-11-02-PargraphVector1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;任务目标：给定一个段落中固定长度的文本内容context，结合当前所处段落一个预测下一个词是什么。&lt;/li&gt;
  &lt;li&gt;方法： 和word2vec相似，每个词用一个向量表示，每个段落也用一个向量表示。连接(concatinate)paragraph vector和所有context的word vector作为输入，预测下一个词。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;paragraph-vector-without-word-ordering-distributed-bag-of-wordspv-dbow&quot;&gt;Paragraph Vector without word ordering: Distributed bag of words(PV-DBOW)&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/img/2017-11-02-PargraphVector2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;任务目标：用paragraph vector预测从当前文本中随机生成的词。&lt;/li&gt;
  &lt;li&gt;方法：每次随机梯度下降的迭代中，选取当前段落中一个固定窗口的文本，从这个窗口的文本中随机选取一个词，then form a classifi-
cation task given the Paragraph Vector.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;final-paragraph-vector&quot;&gt;final paragraph vector&lt;/h2&gt;
&lt;p&gt;combiantion of PV-DM and PV-DBOW.&lt;/p&gt;
&lt;h1 id=&quot;experiments&quot;&gt;Experiments&lt;/h1&gt;

&lt;h2 id=&quot;sentiment-analysis&quot;&gt;Sentiment Analysis&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;数据集： Standford sentiment treebank dataset with 8544 training sentences, 2210 testsentence and 1101 calidation sentences. 每个句子标有0.0到1.0的极性(由人生成的)。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Evaluation:
    &lt;ul&gt;
      &lt;li&gt;5-way fine-grained classification: {Very Negative, Negative, Neutral, Positive,
Very Positive}&lt;/li&gt;
      &lt;li&gt;2-way coarse-grained classification: {Negative, Positive}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;方法：
    &lt;ul&gt;
      &lt;li&gt;training: 先学习训练样本中的sentence和subphrases的向量表示，然后用这些学习得的向量表示学习逻辑回归分类器。&lt;/li&gt;
      &lt;li&gt;testing: 固定所有的词向量，对每个test的句子学习词向量，然后讲句子向量表示输入之前学习得的分类器，对句子做情绪分类。&lt;/li&gt;
      &lt;li&gt;notice: 选词窗口大小为8，向量维度为400。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;结果：
&lt;img src=&quot;/img/2017-11-02-result2.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在多句子数据集IMDB dataset上情绪分析的结果如下：
&lt;img src=&quot;/img/2017-11-02-result2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;information-retrieval&quot;&gt;Information Retrieval&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;数据构造：首先对于每个查询，记录下搜索引擎返回的top10的结果，然后在这些结果数据集上构造数据集如下：对一个查询构造结果三元组，其中前两个是这个查询的答案，另一个不是。 构造的查询结果三元组80% for training, 10% for validation, 10% for testing.&lt;/li&gt;
  &lt;li&gt;测试方法：计算结果三元组中三个paragraph vector之间的距离。测试error是指，在测试三元组中，前两个来自相同查询的结果之间的距离比来第一和第三个来自不同查询结果之间的距离大。&lt;/li&gt;
  &lt;li&gt;结果如下：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/img/2017-11-02-result3.png&quot; alt=&quot;img-w1&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;thinking&quot;&gt;Thinking&lt;/h1&gt;
&lt;p&gt;paragraph vector是很直观的想法。
本文的实验设计很值得借鉴，对于非监督方法而言实验设计尤其中重要。&lt;/p&gt;

&lt;h1 id=&quot;paper-information&quot;&gt;Paper information&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Distributed Representations of Sentences and Documents&lt;/li&gt;
  &lt;li&gt;Quoc Le, Tomas Mikolov&lt;/li&gt;
  &lt;li&gt;Proceedings of the 31st International Conference on Machine Learning, PMLR 32(2):1188-1196, 2014.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1405.4053.pdf&quot;&gt;article link&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;code link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>wencolani</name></author><category term="Paper reading" /><category term="paper" /><category term="paragraph embedding" /><category term="word2vec" /><category term="NLP" /><summary type="html">这篇文章提出了一个非监督学习方法Paragraph Vectore，从文本中学习句子，段落，和文本的向量表示。</summary></entry><entry><title type="html">Some useful meterials for English writing</title><link href="http://localhost:4000/english%20writing/2017/10/24/EnglishWriting/" rel="alternate" type="text/html" title="Some useful meterials for English writing" /><published>2017-10-24T00:00:00+02:00</published><updated>2017-10-24T00:00:00+02:00</updated><id>http://localhost:4000/english%20writing/2017/10/24/EnglishWriting</id><content type="html" xml:base="http://localhost:4000/english%20writing/2017/10/24/EnglishWriting/">&lt;p&gt;这里列举了一些我自己认为比较有用的关于英文写作的资源：&lt;/p&gt;

&lt;h2 id=&quot;books&quot;&gt;Books&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.jlakes.org/ch/web/The-elements-of-style.pdf&quot;&gt;The Element of Style&lt;/a&gt;: 
这是一本美国英语的写作指南。作者是William Strunk, Jr.和E. B. White。书中包括8个“基本使用规则”，10个“创作的基本原则”，“一些形式的问题”，一个包含49个易误用的单词和表达，57个易拼错的单词的列表。在2011年，时代 (杂志)把The Elements of Style列为从1923年到现在100本最富有影响力的书之一。 — 摘自&lt;a href=&quot;https://zh.wikipedia.org/wiki/%E8%8B%B1%E6%96%87%E5%86%99%E4%BD%9C%E6%8C%87%E5%8D%97&quot;&gt;维基百科&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>wencolani</name></author><category term="English Writing" /><category term="English" /><category term="writing" /><category term="English writing" /><summary type="html">这里列举了一些我自己认为比较有用的关于英文写作的资源：</summary></entry><entry><title type="html">Hybrid computing using a neural network with dynamic external memory</title><link href="http://localhost:4000/paper%20reading/2017/10/12/PaperReading/" rel="alternate" type="text/html" title="Hybrid computing using a neural network with dynamic external memory" /><published>2017-10-12T00:00:00+02:00</published><updated>2017-10-12T00:00:00+02:00</updated><id>http://localhost:4000/paper%20reading/2017/10/12/PaperReading</id><content type="html" xml:base="http://localhost:4000/paper%20reading/2017/10/12/PaperReading/">&lt;p&gt;这篇文章提出了一个神经网络框架DNC(Differentiable neural computer)搭载了一个可读可写的外部memory模块。是NTM(Neural Turing Machine)的升级版。&lt;/p&gt;

&lt;p&gt;下面这张论文中的图很好地说明了整个DNC的框架：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/2017-10-12-structure.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;整个框架流程可以分为四个部分：&lt;/p&gt;

&lt;h2 id=&quot;controller&quot;&gt;Controller:&lt;/h2&gt;
&lt;p&gt;controller是一个recurrent的结构，输入输出均为向量，并会产生一系列interface parameters输入到读写部分。整个模型的输入是由controllor接受的。
 整个controllor采用的是deep LSTM结构，含有input gate, forget gate, output gate. controllor输出的output vector和interface vector都由hidden vector生成。&lt;/p&gt;

&lt;h2 id=&quot;read-and-write-head&quot;&gt;Read and write head:&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;read head&lt;/em&gt; 会接受由controllor产生的interface parameters包括：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;read key：和memory的每一行的content计算相似度然用于读操作的寻址，用于计算read weight。&lt;/li&gt;
  &lt;li&gt;read strengths: 每一个read head都会分配一个read strength，可看作是当前时刻t每个read head的读操作权重。&lt;/li&gt;
  &lt;li&gt;read mode: 整个模型定义了三种读的方式,[‘C’]: content lookup using a read key；[‘F’]:reading out location forward in the order they were writteh; [‘B’]: reading out location backword in the order they were written.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每一个read head都会根据controllor的输入和当前的memory生成一个read vector，并反馈给controllor。&lt;/p&gt;

&lt;p&gt;&lt;em&gt;write head&lt;/em&gt; 会接受由controllor产生的interface papameters包括：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;write key: 和read key 类似。&lt;/li&gt;
  &lt;li&gt;write strength: 和read strength相似，但通常write head只有一个，而read head可能有多个。&lt;/li&gt;
  &lt;li&gt;erase vector: 用于控制前一时刻t-1的memory的忘记程度。&lt;/li&gt;
  &lt;li&gt;write vector：和write weight一起控制当前的写入内容。
经过write head 操作后，会更新t-1时刻的momoey并生成时刻t修改后的memory。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;momory&quot;&gt;Momory:&lt;/h2&gt;
&lt;p&gt;Memory 表示成一个N*W的矩阵M。&lt;/p&gt;

&lt;h2 id=&quot;memory-usage-and-temproal-links&quot;&gt;Memory usage and temproal links:&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;usage vector&lt;/em&gt;(各元素取值为0到1之间)用来记录和衡量整个memory各个位置(通常以矩阵的行 为单位)的使用状况，如果当前行的memory usage为1，则不能对此位置进行任何寻址并进行读写操作，必须等待free gate将此位置重新释放。&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;temporal link matrix&lt;/em&gt; 记录的memory中每个位置的写的顺序(图中用黑色的有向箭头表示)。维护这个temporal link matrix的原因是在一些场景下比如序列化的指令(sequence of instruction),比如bAbI数据集上对于对于逻辑推理能力的测试，输入数据的顺序是很重要的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;experiment&quot;&gt;Experiment&lt;/h1&gt;
&lt;p&gt;论文主要包含了三个实验。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Synthetic question answering&lt;/li&gt;
  &lt;li&gt;Graph experiment&lt;/li&gt;
  &lt;li&gt;Block puzzle experiment&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;这里就不详细介绍了&lt;/p&gt;

&lt;h1 id=&quot;thinking&quot;&gt;Thinking&lt;/h1&gt;
&lt;p&gt;个人认为这篇文章最精彩的地方在于整体框架的设计，尤其是对memory的利用，比如如何进行读写操作，读写操作各自都有怎样不同的策略，整个memory的使用有什么上层的策略比如usage等。仔细看论文后面Method部分会体会更深刻。&lt;/p&gt;

&lt;h1 id=&quot;paper-information&quot;&gt;Paper information&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Hybrid computing using a neural network with dynamic external memory&lt;/li&gt;
  &lt;li&gt;Google Deepmind&lt;/li&gt;
  &lt;li&gt;Nature October 2016&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.nature.com/nature/journal/v538/n7626/abs/nature20101.html?foxtrotcallback=true&quot;&gt;article link&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/deepmind/dnc&quot;&gt;code link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>wencolani</name></author><category term="Paper reading" /><category term="paper" /><category term="nature" /><category term="memory" /><category term="nature" /><summary type="html">这篇文章提出了一个神经网络框架DNC(Differentiable neural computer)搭载了一个可读可写的外部memory模块。是NTM(Neural Turing Machine)的升级版。</summary></entry><entry><title type="html">新的演员</title><link href="http://localhost:4000/%E9%9A%8F%E7%AC%94/2017/10/07/suibi/" rel="alternate" type="text/html" title="新的演员" /><published>2017-10-07T00:00:00+02:00</published><updated>2017-10-07T00:00:00+02:00</updated><id>http://localhost:4000/%E9%9A%8F%E7%AC%94/2017/10/07/suibi</id><content type="html" xml:base="http://localhost:4000/%E9%9A%8F%E7%AC%94/2017/10/07/suibi/">&lt;p&gt;每条路都是花枝招展&lt;/p&gt;

&lt;p&gt;可为什么&lt;/p&gt;

&lt;p&gt;路上都是花仙子&lt;/p&gt;

&lt;p&gt;那就走近看一看&lt;/p&gt;

&lt;p&gt;哦 原来都是敬业的演员&lt;/p&gt;

&lt;p&gt;恍然大悟&lt;/p&gt;

&lt;p&gt;回头无岸&lt;/p&gt;

&lt;p&gt;新的演员&lt;/p&gt;</content><author><name>wencolani</name></author><category term="随笔" /><category term="随笔" /><category term="小诗" /><summary type="html">每条路都是花枝招展</summary></entry><entry><title type="html">Knowledge Transfer for Out-of-Knowledge-Base Entities, A Graph Neural Network Approach</title><link href="http://localhost:4000/paper%20reading/2017/09/26/PaperReading/" rel="alternate" type="text/html" title="Knowledge Transfer for Out-of-Knowledge-Base Entities, A Graph Neural Network Approach" /><published>2017-09-26T00:00:00+02:00</published><updated>2017-09-26T00:00:00+02:00</updated><id>http://localhost:4000/paper%20reading/2017/09/26/PaperReading</id><content type="html" xml:base="http://localhost:4000/paper%20reading/2017/09/26/PaperReading/">&lt;p&gt;这篇文章提出了一个新的问题：关于out-of-knowledge-base(OOKB) entity的预测。以往的knowledge base embedding方法做链接预（link prediction）测和三元组分类(triple classification)时所用的测试数据集里都假设所有的实体的关系都已经在训练数据集里出现过，即每个实体和关系都有相关的向量表示，如果出现新的实体，那么模型需要进行重新训练。&lt;/p&gt;

&lt;p&gt;out-of-knowlege-base entity 的问题如下：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;out-of-knowledge-base entity problem arises when new entity(OOKB entities) occur in the relation triplets that are given to the system after training.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;解决OOKB的问题主要在于如何从已有的实体和关系的向量表示里获得OOKB实体的向量表示，本文主要利用了OOKB实体和知识库中已有的实体的链接关系得到其向量表示。&lt;/p&gt;

&lt;h2 id=&quot;proposed-model&quot;&gt;Proposed Model&lt;/h2&gt;

&lt;p&gt;本文借鉴了[Graph-NN]的方法, 将生成OOKB entity的向量表示的过程分为propagation model 和 output model.&lt;/p&gt;

&lt;h3 id=&quot;propagation-model&quot;&gt;Propagation model&lt;/h3&gt;

&lt;p&gt;Propagation model 分为两个部分： trandition function 和 pooling function.&lt;/p&gt;

&lt;h4 id=&quot;transition-function&quot;&gt;transition function&lt;/h4&gt;

&lt;p&gt;transition function 的主要目标是通过每个与OOKB实体具有链接关系的三元组中已有的实体和关系推测出当前OOKB实体的向量表示。具体定义如下：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$ T(\textbf{v}) = \textbf{v} $ — (identity)&lt;/li&gt;
  &lt;li&gt;$ T(\textbf{v}) = tanh(\textbf{Av}) $ — (single tanh layer)&lt;/li&gt;
  &lt;li&gt;$ T(\textbf{v}) = ReLU(\textbf{Av}) $ — (single ReLU layer)
这里的transition function 也可以换成其他的神经网络，比如batch-normalization、residual connection 和 LSTM等。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;注意参数A的定义可以不是一个固定的全局的参数，而是与当前OOKB实体参与的三元组中的关系相关的 (注：这样肯定能提高效果的). 在single ReLU layer 中本文也添加了batch normalization.&lt;/p&gt;

&lt;h4 id=&quot;pooling-function&quot;&gt;pooling function&lt;/h4&gt;

&lt;p&gt;pooling function 主要目的是通过多个OOKB entity的表示结果生成一个向量表示，pooling的方法主要有以下三种：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;sum pooling: &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt; \\( P(S) = \sum_{i=1}^N x_i \\)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;acerage pooling:&lt;/li&gt;
  &lt;li&gt;max pooling&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(文中也使用了stacking and unrolling，不过不太了解)&lt;/p&gt;

&lt;h3 id=&quot;output-model&quot;&gt;output model&lt;/h3&gt;
&lt;p&gt;output model借用了TransE的方法，使用|h + r -t| 作为score function，并且采用margin-based objective function. 相对于之前TrasnE的pairwise-margin，本文提出了absolute-margin，这样的定义同样可以使得可以生成更多的负样本，可以提高最终embedding的效果。&lt;/p&gt;

&lt;h2 id=&quot;experiment&quot;&gt;Experiment&lt;/h2&gt;
&lt;p&gt;本文也进行了常规的knowledge base embedding的三元组分类的实验，即没有OOKB实体存在，在WN11和FB13上的实验均比TransE要好一些。但是我更关注OOKB实体的问题，所以主要介绍关于OOKB实体的实验。&lt;/p&gt;
&lt;h3 id=&quot;数据集的构造&quot;&gt;数据集的构造&lt;/h3&gt;
&lt;p&gt;本文实验数据集的构造过程非常值得借鉴。以WordNet11s数据集为例， 构造过程如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;选取OOKBentity：从WN11的test数据集中选取N=1000, 3000, 5000三元组。然后有三种策略选取OOKB entity：
    &lt;ol&gt;
      &lt;li&gt;将选取出的三元组中的所有head entity当作是OOKB实体&lt;/li&gt;
      &lt;li&gt;将选取出的三元组中的所有tail entity当作是OOKB实体&lt;/li&gt;
      &lt;li&gt;将选取出的三元组中的所有head entity 和 tail entity均当作OOKB实体
然后去掉候选OOKB实体中与任何非OOKB实体都没有链接的实体。&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;将原来训练数据集中的三元组划分入两个类别：将不含有选出的OOKB实体的三元组留在训练数据集中，将含有选出的OOKB实体的三元组放入auxiliary set中。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;由于OOKB entity是新提出的问题，没有baseline， 本文自己构造了baseline： 通过对TransE在训练数据集上的训练结果，直接选取和OOKB相连接的实体的向量表示并进行三种不同方式的pooling。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/2017-09-26-result.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;paper-information&quot;&gt;Paper information&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Knowledge Transfer for Out-of-Knowledge-Base Entities: A Graph Neural Network Approach&lt;/li&gt;
  &lt;li&gt;Takuo Hamaguchi, Hidekazu Oiwa, Masashi Shimbo, and Yuji Matsumoto&lt;/li&gt;
  &lt;li&gt;IJCAI 2017&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.ijcai.org/proceedings/2017/0250.pdf&quot;&gt;article link&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/takuo-h/GNN-for-OOKB&quot;&gt;code link&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>wencolani</name></author><category term="Paper reading" /><category term="paper" /><category term="incremental" /><category term="KG embedding" /><summary type="html">这篇文章提出了一个新的问题：关于out-of-knowledge-base(OOKB) entity的预测。以往的knowledge base embedding方法做链接预（link prediction）测和三元组分类(triple classification)时所用的测试数据集里都假设所有的实体的关系都已经在训练数据集里出现过，即每个实体和关系都有相关的向量表示，如果出现新的实体，那么模型需要进行重新训练。</summary></entry><entry><title type="html">Vim command</title><link href="http://localhost:4000/command%20learning/2017/09/24/VimCommand/" rel="alternate" type="text/html" title="Vim command" /><published>2017-09-24T00:00:00+02:00</published><updated>2017-09-24T00:00:00+02:00</updated><id>http://localhost:4000/command%20learning/2017/09/24/VimCommand</id><content type="html" xml:base="http://localhost:4000/command%20learning/2017/09/24/VimCommand/">&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;vimtutor&lt;/code&gt; — an interactive tutorial in UNIX version vim&lt;/p&gt;

&lt;h2 id=&quot;under-the-normal-mode&quot;&gt;under the normal mode:&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;i&lt;/code&gt; — insert mode: insert a character before the character under the cursor&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;a&lt;/code&gt; — insert mode: insert a character after the character under the cursor&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;o&lt;/code&gt; — inserr mode: open a new line and put hte cursor to the begining of the new line&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; — delete current letter&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;enter&amp;gt;&lt;/code&gt; — move to the begining of the next line&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;h&lt;/code&gt; — move left&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;j&lt;/code&gt; — move down&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;k&lt;/code&gt; — move up&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;l&lt;/code&gt; — move right&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;u&lt;/code&gt; — undo the last operation&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;:&lt;/code&gt; — enter command mode&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dd&lt;/code&gt; — delete a line&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;w&lt;/code&gt; — move cursor forward one word&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;b&lt;/code&gt; — move cursor backward one word&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;$&lt;/code&gt; — move to the end of the line(also e.g.3$)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;^&lt;/code&gt; — move to the first nonblank character of the line&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;fx&lt;/code&gt; — searches the right line for the single character x&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Fx&lt;/code&gt; — searches the left line for the single character x&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tx&lt;/code&gt; — searches the right line for this single character x and stop one character before x (search till)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Tx&lt;/code&gt; — for the left side&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;f&amp;lt;esc&amp;gt;&lt;/code&gt; — abord forward search&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;G&lt;/code&gt; — move the last line&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;3G&lt;/code&gt; — move to 3 line&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;1G&lt;/code&gt; — move to the top of the file&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;%&lt;/code&gt; — move to the first line&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;50%&lt;/code&gt; — move to the half part of the file&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;75%&lt;/code&gt; — move to the 75% of the file&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;CTRL+g&lt;/code&gt; — show where you are in the file&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;CTRL+u&lt;/code&gt; — scroll up half a screen of text&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;CTRL+d&lt;/code&gt; — scroll down hald a screen of the text&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;v&lt;/code&gt; — visual mode&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dw&lt;/code&gt; — delete the rest of the current word where the cursor in&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;d$&lt;/code&gt; — delete the current line from where the cursor in&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;3dw&lt;/code&gt; — delete one word three times&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;d3w&lt;/code&gt; — delete three word one time&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;3d2w&lt;/code&gt; — delete two words three times&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;c&lt;/code&gt; — change text, it deletes the current choosed(in visul mode) otherwise the whole line  and leave you in the insert mode&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cw&lt;/code&gt; — change the current word&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;cc&lt;/code&gt; — change the whole line (as dd)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;c$(or C)&lt;/code&gt; — change from current place to the end of the line&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;.&lt;/code&gt; — repeat the last delete or change command&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;J&lt;/code&gt; — join the current with the next line (also 3J)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;rx&lt;/code&gt; — replace current character with x (also 3rx)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;3rx&lt;/code&gt; — replace threee character with x&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;3r&amp;lt;Enter&amp;gt;&lt;/code&gt; — replace 3 characters with one [Enter]&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;~&lt;/code&gt; — change a character’s case, changes uppercase to lowercase and vice versa&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;qx&lt;/code&gt; — record keystrokes into the register named x   /  — use q to quit the record  / — use @x to reuse the operation / — the difference from qx to . is that qx could record multiple steps&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;under-the-command-mode&quot;&gt;under the command mode&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;q!&lt;/code&gt; — quit without save the changes&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;:h(help)&lt;/code&gt; — help document&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;:h x&lt;/code&gt; — get help on x commond&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;:set number&lt;/code&gt; — show the line number&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;:set nonumber&lt;/code&gt; — turn off showing the number&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;:help v_d&lt;/code&gt; — get the help about what the delete(d) command does in visual mode&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;under-the-visual-mode&quot;&gt;under the visual mode&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;J&lt;/code&gt; — join the selected lines&lt;/li&gt;
&lt;/ul&gt;</content><author><name>wencolani</name></author><category term="Command learning" /><category term="vim" /><summary type="html">vimtutor — an interactive tutorial in UNIX version vim</summary></entry><entry><title type="html">《曾文正公家书 上》</title><link href="http://localhost:4000/book%20reading/2017/05/30/ZengWenZhengGongJiaShu1/" rel="alternate" type="text/html" title="《曾文正公家书 上》" /><published>2017-05-30T00:00:00+02:00</published><updated>2017-05-30T00:00:00+02:00</updated><id>http://localhost:4000/book%20reading/2017/05/30/ZengWenZhengGongJiaShu1</id><content type="html" xml:base="http://localhost:4000/book%20reading/2017/05/30/ZengWenZhengGongJiaShu1/">&lt;p&gt;初读此书觉得有几分枯燥，书中所记不过是琐碎的日常。但慢慢看着看着逐渐感受到了鲜活的形象，感受到了何为“长兄如父”“君君臣臣，父父子子”。历史中再伟岸的形象都是由鲜明及琐碎的生活构成的，伟人不是不食人间烟火，横空出世的圣人。他们也有自己所生活的时代的温度和生活的烙印。这是自己读的第一本历史一手书籍，值得。&lt;/p&gt;

&lt;p&gt;每次看到书信的结尾处写着“兄国藩手草”五个大字，都觉得无比羡慕，我多么希望也有这样的长兄啊，那些文字间的提点皆珠玑。作为长兄的曾国藩操心的事儿可真不少呢，上至国事下至家务，无不上心。对几个弟弟们的成长也是关怀备至，会按照他们不同的天赋安排学习进度和学习策略，会经常督促他们发给自己最新的学习成果，当然以习字和作文为主。真是羡煞我也。&lt;/p&gt;

&lt;p&gt;曾国藩对人都有自己的判断，处于何境何人可用何人不可用心中一清二楚，虽处事文雅，但内心绝不含糊。何人为有恩之人也一直铭记于心，无论自己处境如何都不忘感恩。读了这么多书信在平实的生活中逐渐构造出了曾国藩这个有血有肉的立体形象。至爽至爽。&lt;/p&gt;

&lt;p&gt;说一说对曾国藩本人之外的认识吧。首先是对历史的认识，每个人的认知都是受到当前历史环境的局限的，在科学还并不发达的清朝，曾国藩也会有一些愚昧的认知，比如家庭的命运和祖坟的选址有关，信人但更信天。对医药的认知也是不清楚其根本。曾国藩常常会在信中癣疾又发，且数月不愈，这在现在看来真是不可思议，如果当今又连发数月的癣疾，当事人肯定急得团团转，不得不感谢现代的医学进步，让我们免去了多少痛苦啊。&lt;/p&gt;

&lt;p&gt;那时的通讯并不发达，主要依靠书信沟通，但效率极慢，一封家信寄到家至少半个月之久，再待家人回信，一来一回已是一月过去了。所以每封信的第一段总是先告诉对方自己已经寄出的信件有哪些，收到的家中的信件有几封，多么原始而简单粗制的沟通方式啊，想想我们现在有微信是多么大的进步啊！但诸事均为双刃剑，由于信息沟通无法及时，他们往往计划事情总以周、月来计，他们可以预料到相对久远的事情，但我们现代人很多做不到，由于信息沟通足够及时，我们的计划总是会被突发状况打乱，渐渐地我们养成了以天、周为计划，但很少能够遇见待一月或半年一年之后的事，一来也是因为变化太快无法预见，二来也是应为我们渐渐遗忘了这样的能力——通过对历史的洞见去总结和预测未来可能发生的事情。通讯效率的差异也造成了我们用人策略的不同，在以前通讯不及时，远水救不了近火确实是真理，作为用人者最核心的策略是选择自己足够信任的人并且完全信任他去完成重要的事情，那是用人的眼光更重要。但现在不一样，由于通讯足够及时，现在用人的核心策略是选择有足够能力完成这样件事的人，而不再是有能力统领这件事完成的人。多么奇妙啊，技术的进步逐渐(无形)改变了我们的生存方式。而且技术会变的越来越重要，因为技术在改造这个构建在信息不对等基础上的世界，技术应该让世界变得更加公平。&lt;/p&gt;

&lt;p&gt;还有对战争的认识，由于太平天国起义，国家有十几年(1851-1864)处于战乱年代，借用书信中的说法就是“乱世”，当战争来临，很多事都将化为泡影，那些美好的事情重见天日必等战争结束之时，呜呼，哀哉。我曾经幻想过自己生在战火纷飞的年代，一定是一条铮铮铁骨，但此刻起，再无心之往向。战争太过粗暴，摧毁了太多脆弱的美好，因为美好，即便是脆弱的，我们也应该去守护，心存敬畏，不是吗？&lt;/p&gt;

&lt;p&gt;历史的一手书籍值得被品读。&lt;/p&gt;</content><author><name>wencolani</name></author><category term="Book reading" /><category term="book reading" /><summary type="html">初读此书觉得有几分枯燥，书中所记不过是琐碎的日常。但慢慢看着看着逐渐感受到了鲜活的形象，感受到了何为“长兄如父”“君君臣臣，父父子子”。历史中再伟岸的形象都是由鲜明及琐碎的生活构成的，伟人不是不食人间烟火，横空出世的圣人。他们也有自己所生活的时代的温度和生活的烙印。这是自己读的第一本历史一手书籍，值得。</summary></entry></feed>