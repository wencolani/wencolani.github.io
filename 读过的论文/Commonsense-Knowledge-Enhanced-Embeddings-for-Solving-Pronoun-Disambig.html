<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Commonsense Knowledge Enhanced Embeddings for Solving Pronoun Disambiguation Problems in Winograd Schema Challenge</title>
    <style type="text/css" media="all">
      body {
        margin: 0;
        font-family: "Helvetica Neue", Helvetica, Arial, "Hiragino Sans GB", sans-serif;
        font-size: 14px;
        line-height: 20px;
        color: #777;
        background-color: white;
      }
      .container {
        width: 700px;
        margin-right: auto;
        margin-left: auto;
      }

      .post {
        font-family: Georgia, "Times New Roman", Times, "SimSun", serif;
        position: relative;
        padding: 70px;
        bottom: 0;
        overflow-y: auto;
        font-size: 16px;
        font-weight: normal;
        line-height: 25px;
        color: #515151;
      }

      .post h1{
        font-size: 50px;
        font-weight: 500;
        line-height: 60px;
        margin-bottom: 40px;
        color: inherit;
      }

      .post p {
        margin: 0 0 35px 0;
      }

      .post img {
        border: 1px solid #D9D9D9;
      }

      .post a {
        color: #28A1C5;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <div class="post">
        <h1 class="title">Commonsense Knowledge Enhanced Embeddings for Solving Pronoun Disambiguation Problems in Winograd Schema Challenge</h1>
        <div class="show-content">
          <p>作者：Quan Liua, Hui Jiang...</p><p>A Winograd schema question is a pair of sentences that differ only in one to two words which results in a different resolution of coreference.<br></p><p>the commonsense knowledge would be quantized as semantic constraints to guids the semantic word embedding training process.用常识知识来指导word embedding的获得</p><p>the process to answer PDP problems could be fullfilled by directly calculating the semantic similarities between the representation vectors of the pronoun under concern and all candidate mentions.最终将PDP问题看作一个向量相似度计算的问题。</p><p>PDP task does't have training data.所以要么采用无监督的方法训练，要么想办法获得训练数据。</p><p>PDP问题不涉及三个句子及以上的例子。</p><p>The difficulties of solving the comples PDP problems:1 The lack of training data. 2 The requirement of commonsense reasoning.</p><p>作者将PDP问题当作一个典型的机器学习的问题，但是面临没有训练数据的问题，所以最后选择了设计一个problem solver.</p><p>Commonsense knowledge in Cyc are represented by formal language.&nbsp;</p><p>实验中使用的常识知识库有三个：ConceptNet, WordNet and CauseCom.</p><p>ConcepNet: Similarities between linked concepts should be larger than the similarities between unlinked concepts.</p><p>WordNet: 1)similarities between a word and its synonymous words are larger than similsrities between the word and its antonymous words. 2)Similarities of words that belong to the same semantic category would be larger than similarities of words that belong to different categories. 3)Similarities between words that have shorter distances in a semantic hierarchy should be larger than similarities of words that have longer distance.</p><p>CauseCom: similarity based on wither one word effect the other word. CauseCom里主要包含了verbs and adjectives, not include nouns, adverbs and prepositions.</p><p>word embedding 采取的是skip-gram model，and regard commonsense knowledge as a penalty term.&nbsp;</p><p>训练word embedding 的语料为：1) CBTest: a book corpus, containing 300 million tokensand and 53541 words 2)Wikipedia: 1 bilion tokens and 235167 words. \\</p><p>The embedding dimention is 100</p><p>The window size is 5.</p><p>The penalty term combination coefficient beta is 0.01.</p><p>The learning rate for SGD is 0.025.</p><p>This paper uses the popular coreference resolution datasets: OntoNotes , to extract labelled mention pairs for model training.</p><p>目前PDP问题最好的准确率是论文中的66.7%。</p><p>评论：目前解决PDP的关键在于相关常识知识的获取。</p>
        </div>
      </div>
    </div>
  </body>
</html>
